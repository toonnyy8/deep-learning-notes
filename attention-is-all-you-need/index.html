<!DOCTYPE html>
<html>

<head>
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Noto+Sans+TC&display=swap');
        @import url('https://fonts.googleapis.com/css2?family=Source+Code+Pro&display=swap');

        body {
            font-family: 'Source Code Pro', 'Noto Sans TC', sans-serif;
            background-color: #252525;
            color: #FBF1C7;
            margin-left: 25%;
            margin-right: 25%;
            font-size: 1rem;
        }


        .container {
            max-width: 70rem;
            min-width: 25rem;
        }
    </style>
    <style>
        @import url(../import/main.css);
    </style>
</head>

<body>
    <div class="container">
        <h1><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></h1>
        <h2>簡介</h2>
        <p>
            在 2017 年，由 self attention 構成的新模型 Transformer 被提出。<br />
            最初使用在文本翻譯的研究上，從而推廣至整個 NLP 領域<br />
            但實際上，只要能將問題轉換成<strong>「N個一維張量」</strong>，就能使用 Transformer 處理。
        </p>
        <h4>優點</h4>
        <ul>
            <li><strong>快速</strong>：跟 RNN 類的模型相比，可將輸入序列做平行運算。</li>
            <li><strong>長距離關注</strong>：跟 CNN 類的模型相比，可依據輸入序列長度動態改變感受野。</li>
        </ul>
        <h4>缺點</h4>
        <ul>
            <li><strong>記憶體使用量過大</strong>，受硬體設備限制了其關注距離。</li>
        </ul>

        <h2>架構</h2>
        <a href="./img/transformer.png" ta><img src="./img/transformer.png" alt="Transformer 架構圖" /></a>
        <p>
            Transformer 主要由「Encoder」以及「Decoder」兩區塊組成，<br />
            <strong>Encoder</strong> 負責精煉 Input 的特徵，<br />
            <strong>Decoder</strong> 則是關注 Input 與 Target 的關聯性，並藉此取得所需的資訊。
        </p>
        <h3></h3>
    </div>
</body>

</html>