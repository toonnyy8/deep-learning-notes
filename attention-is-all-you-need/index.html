<!DOCTYPE html>
<html>

<head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <style>
        @import url(../import/main.css);
    </style>
</head>

<body>
    <!-- <div class="container"> -->
    <h1><a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></h1>
    <ul class="directory">
        <li>
            <a href="#簡介">簡介</a>
        </li>
        <li>
            <a href="#架構">架構</a>
        </li>
        <li>
            <a href="#在翻譯任務的訓練與使用方式">在翻譯任務的訓練與使用方式</a>
        </li>
        <li>
            <a href="#實驗">實驗</a>
        </li>
    </ul>
    <div class="block">
        <h2 id="簡介">簡介</h2>
        <div class="block">
            <p>
                在 2017 年，由 self attention 構成的新模型 Transformer 被提出。<br />
                最初使用在文本翻譯的研究上，從而推廣至整個 NLP 領域。<br />
                但實際上，只要能將問題轉換成<strong>「N個一維張量」</strong>，就能使用 Transformer 處理。
            </p>
            <h4>優點</h4>
            <ul>
                <li><strong>快速</strong>：跟 RNN 類的模型相比，可將輸入序列做平行運算。</li>
                <li><strong>長距離關注</strong>：跟 CNN 類的模型相比，可依據輸入序列長度動態改變感受野。</li>
            </ul>
            <h4>缺點</h4>
            <ul>
                <li><strong>記憶體使用量過大</strong>，受硬體設備限制了其關注距離。</li>
            </ul>
        </div>

        <h2 id="架構">架構</h2>
        <ul class="directory">
            <li><a href="#Multi-Head-Self-Attention">Multi Head Self Attention</a></li>
            <li><a href="#Input-&-Target">Input & Target</a></li>
            <li><a href="#Embedding">Embedding</a></li>
            <li><a href="#Positional-Encoding">Positional Encoding</a></li>
            <li><a href="#Encoders-&-Decoders">Encoders & Decoders</a></li>
        </ul>
        <div class="block">
            <div class="block">
                <p>
                    Transformer 主要由「Encoders」以及「Decoders」兩區塊組成，<br />
                    <strong>Encoders</strong> 負責利用上下文資訊精煉 Input 的特徵，<br />
                    <strong>Decoders</strong> 則是關注 Input 與 Target 的關聯性，並藉此取得所需的資訊。
                </p>
                <a href="./img/transformer.png"><img src="./img/transformer.png" alt="Transformer 架構圖" /></a>
                <p>
                    Encoder 與 Decoder 的內部機制便是本論文的重點：<strong>Multi Head Self Attention</strong>。
                </p>
            </div>

            <h3 id="Multi-Head-Self-Attention">Multi Head Self Attention</h3>
            <div class="block">
                <div class="block">
                    <p>
                        Multi Head Attention 是由多個 <strong>Scaled Dot Product Attention</strong> 組成。<br />
                        每個 Scaled Dot Product Attention 會計算出不同的 <strong>Head</strong>，<br />
                        Head 就像是 CNN 的 <strong>Feature Map</strong> 一樣，<br />
                        各 Head 具有自己關注的特徵，從越多方面取得特徵便能獲得更好的效果。
                    </p>
                    <a href="./img/multi-head-attention.png"><img src="./img/multi-head-attention.png"
                            alt="Transformer 架構圖" /></a>
                </div>
                <h4>Scaled Dot Product Attention</h4>
                <div class="block">
                    Attention Function 如下式
                    <p class="math">
                        <img class="math-xl" src="./math/scale-dot-product-attention.png"
                            alt="Attention(Q,K,V)=Softmax((Q*K^T)/(depth_k^0.5))*V" />
                    </p>
                    <p>其中</p>
                    <p class="math">
                        <img class="math-2xl" src="./math/QKV.png"
                            alt="Attention(Q,K,V)=Softmax((Q*K^T)/(depth_k^0.5))*V" />
                    </p>
                    <p>
                        <strong>Q<sub>in</sub></strong>/<strong>K<sub>in</sub></strong>/<strong>V<sub>in</sub></strong>
                        矩陣都是由一句話傳換成的詞向量。<br />
                        而其中 <strong>K<sub>in</sub> 與 V<sub>in</sub> 是相同的矩陣</strong>。
                    </p>
                    <p>
                        大小分別為：
                    <ul>
                        <li>Q<sub>in</sub>：length Q x d<sub>embedding</sub></li>
                        <li>K<sub>in</sub>：length K x d<sub>embedding</sub></li>
                        <li>V<sub>in</sub>：length K x d<sub>embedding</sub></li>
                    </ul>
                    </p>
                    <a href="./img/k_input-v_input-q_input.png">
                        <img src="./img/k_input-v_input-q_input.png" alt="Transformer 架構圖" />
                    </a>
                    <blockquote>
                        上圖為 Q<sub>in</sub>、K<sub>in</sub>、V<sub>in</sub> 的例子
                    </blockquote>
                    <p>
                        Q<sub>in</sub>/K<sub>in</sub>/V<sub>in</sub> 經過各自的線性轉換後變成
                        <strong>Q</strong>/<strong>K</strong>/<strong>V</strong>
                        矩陣：
                    </p>
                    <a href="./img/attention-pre-linear.png">
                        <img src="./img/attention-pre-linear.png" alt="Transformer 架構圖" />
                    </a>
                    <p>
                        另外，<strong>Q</strong>/<strong>K</strong>/<strong>V</strong> 轉換完後的大小分別為
                    <ul>
                        <li>Q：length Q x d<sub>k</sub></li>
                        <li>K：length K x d<sub>k</sub></li>
                        <li>V：length K x d<sub>v</sub></li>
                    </ul>
                    </p>
                    <p>
                        轉換完成後就進入 Attention Function。<br />
                        利用內積計算 Q 的每個向量與 K 的每個向量的相似程度，若向量夾角越小，便會得到較大的內積，<br />
                        因此先將 Q 與 K 的轉置矩陣 K^T 相乘得出 QK 舉陣：<br />
                        (等同計算 Q 的每個向量與 K 的每個向量的內積)
                    </p>
                    <a href="./img/Scaled-Dot-Product-Attention/0.png">
                        <img src="./img/Scaled-Dot-Product-Attention/0.png" alt="Transformer 架構圖" />
                    </a>
                    <p>
                        再將 QK 矩陣除以 d<sub>k</sub>^0.5 後帶入 Softmax 得到注意力權重：<br />
                        (除以 d<sub>k</sub>^0.5 是為了避免注意力權重只剩下接近 0 與 1
                        的數值，如果沒有進行縮放會使計算所得之梯度非常小，造成梯度消失而無法更新權重。)
                    </p>
                    <a href="./img/Scaled-Dot-Product-Attention/1.png">
                        <img src="./img/Scaled-Dot-Product-Attention/1.png" alt="Transformer 架構圖" />
                    </a>
                    <p>
                        計算出注意力權重後，便可利用 V 來建立出 Head：
                    </p>
                    <a href="./img/Scaled-Dot-Product-Attention/2.png">
                        <img src="./img/Scaled-Dot-Product-Attention/2.png" alt="Transformer 架構圖" />
                    </a>
                </div>
                <h4>Multi Head</h4>
                <div class="block">
                    <p class="math">
                        <img class="math-4xl" src="./math/multi-head-attention.png"
                            alt="Attention(Q,K,V)=Softmax((Q*K^T)/(depth_k^0.5))*V" />
                    </p>
                    <p>
                        Multi Head Attention 就是將多個 Scaled Dot Product Attention 的結果串聯後經過一次線性轉換。<br />
                        線性轉換後的大小為 <strong>length Q x d<sub>embedding</sub></strong>，與輸入的 Q<sub>in</sub> 大小一致。
                    </p>
                    <a href="./img/Multi-Head-Attention/0.png">
                        <img src="./img/Multi-Head-Attention/0.png" alt="Transformer 架構圖" />
                    </a>
                </div>
                <h4>在向量空間中的意義</h4>
                <div class="block">
                    <a href="./img/QKV/0.png">
                        <img src="./img/QKV/0.png" alt="Transformer 架構圖" />
                    </a>
                    <a href="./img/QKV/1.png">
                        <img src="./img/QKV/1.png" alt="Transformer 架構圖" />
                    </a>
                    <a href="./img/QKV/2.png">
                        <img src="./img/QKV/2.png" alt="Transformer 架構圖" />
                    </a>
                    <a href="./img/QKV/3.png">
                        <img src="./img/QKV/3.png" alt="Transformer 架構圖" />
                    </a>
                    <a href="./img/QKV/4.png">
                        <img src="./img/QKV/4.png" alt="Transformer 架構圖" />
                    </a>
                    <a href="./img/QKV/5.png">
                        <img src="./img/QKV/5.png" alt="Transformer 架構圖" />
                    </a>
                    <a href="./img/QKV/6.png">
                        <img src="./img/QKV/6.png" alt="Transformer 架構圖" />
                    </a>
                    <a href="./img/QKV/7.png">
                        <img src="./img/QKV/7.png" alt="Transformer 架構圖" />
                    </a>
                    <a href="./img/QKV/8.png">
                        <img src="./img/QKV/8.png" alt="Transformer 架構圖" />
                    </a>
                    <a href="./img/QKV/9.png">
                        <img src="./img/QKV/9.png" alt="Transformer 架構圖" />
                    </a>
                </div>
            </div>

            <h3 id="Input-&-Target">Input & Target</h3>
            <div class="block"></div>

            <h3 id="Embedding">Embedding</h3>
            <div class="block">
                <a href="./img/word-vector-space_clip.mp4">
                    <video class="w-5/6 m-auto" autoplay="" muted="" loop="" data-paused-by-reveal="">
                        <source src="./img/word-vector-space_clip.mp4" type="video/mp4">
                    </video>
                </a>
                <a href="./img/embedding_2.png"><img src="./img/embedding_2.png" alt="Transformer 架構圖" /></a>
            </div>

            <h3 id="Positional-Encoding">Positional Encoding</h3>
            <div class="block">
                <p>
                    在全部由 Self Attention 構成的 Transformer 中，<strong>無法</strong>做到如同 RNN 與 CNN 那樣自然的掌握位置資訊。<br />
                    但是處理序列問題時，<strong>位置資訊又非常重要</strong>，因此使用了 <strong>Positional Encoding</strong>
                    的方式<strong>手動的將訊息寫進輸入之中</strong>。
                </p>
                <p class="math">
                    <img class="math-sm" src="./math/positional-encoding_0.png"
                        alt="PositionalEncoding(Input)=Input+PE" />
                </p>
                <p>
                    上式的 <strong>PE</strong> 就是記載著位置資訊的矩陣，其內容如下式：
                </p>
                <p class="math">
                    <img class="math-lg" src="./math/positional-encoding_1.png"
                        alt="PE[pos,2i]=sin(pos/10000^(2i/d_embedding)),PE[pos,2i+1]=cos(pos/10000^((2i+1)/d_embedding))" />
                </p>
                <P>
                    其中的 <strong>pos</strong> 與 <strong>2i、2i+1</strong> 意義如下：
                </P>
                <ul>
                    <li>pos：句子中的第 pos 個字詞。</li>
                    <li>2i、2i+1：word embedding 後所得向量的第 n 維。</li>
                </ul>
                <a href="./img/positional-encoding.png"><img src="./img/positional-encoding.png"
                        alt="positional encoding 範例" /></a>
                <blockquote>
                    上圖是 50 個字詞、d<sub>embedding</sub> = 10 的 PE 範例
                </blockquote>
            </div>

            <h3 id="Encoders-&-Decoders">Encoders & Decoders</h3>
            <div class="block">
                <div class="block">
                    <p>
                        Encoders Decoders 是 Transformer 的核心區塊，<br />
                        他們個別由多層的 Encoder Layer 與 Decoder Layer 相疊而成，<br />
                        每層都具有各自的可訓練權重。
                    </p>
                    <p>
                        當 Input 經過 Encoders 後，就會做為 Decoders 的 K/V ，將 Input 的資訊傳遞過去。
                    </p>
                    <a href="./img/encoders-decoders.png"><img src="./img/encoders-decoders.png"
                            alt="Encoders & Decoders 由多個 encoder layer 與 decoder layer 組成" /></a>

                </div>
                <h4>Encoder Layer</h4>
                <div class="block">
                    <a href="./img/encoder-layer.png"><img src="./img/encoder-layer.png" alt="Encoder" /></a>
                    <p>
                        Encoder Layer 中有兩塊主要計算結構，<br />
                        分別是前面所提及的 <strong>Multi Head Attention</strong><br />
                        以及由幾層全連接層(中間有使用 ReLU 作為激活函數)組成的 <strong>Feed Forward</strong>。
                    </p>
                    <h5>Multi Head Attention</h5>
                    <div class="block">
                        <p>
                            在 Encoder Layer 的 Multi Head Attention 中，會傳入相同的
                            Q<sub>in</sub>、K<sub>in</sub>、V<sub>in</sub>。
                        </p>
                    </div>
                    <h5>Residual</h5>
                    <div class="block">
                        <p>
                            透過 Residual 的跳接結構可以保留最開始的 Positional Encoding，<br />
                            因此不必每通過一層 Encoder Layer 就要補上位置資訊，<br />
                            對於深層的網路也能有效的避免梯度消失。
                        </p>
                        <a href="./img/residual.png"><img src="./img/residual.png" alt="residual" /></a>
                    </div>
                    <h5>Layer Normalization</h5>
                    <div class="block">
                        Layer Normalization 會對每個 Word Embedding Vector 做 <strong>Z 分數標準化</strong>，<br />
                        使其平均值 = 0、標準差 = 1，然後將標準化的結果與一組縮放、偏移權重做計算，便是 Layer Normalization 的最後答案。
                    </div>
                </div>
                <h4>Decoder Layer</h4>
                <div class="block">
                    <a href="./img/decoder-layer.png"><img src="./img/decoder-layer.png" alt="Encoder" /></a>
                </div>
            </div>
        </div>

        <h2 id="在翻譯任務的訓練與使用方式">在翻譯任務的訓練與使用方式</h2>
        <ul class="directory">
            <li><a href="#訓練">訓練</a></li>
            <li><a href="#使用方式">使用方式</a></li>
        </ul>
        <div class="block">
            <h3 id="訓練">訓練</h3>
            <div class="block">
                <a href="./img/black_box.png"><img src="./img/black_box.png"
                        alt="Transformer 的輸入有兩個，一個是 Input、另一個是 Target，而輸出的詞字數量則會與 Target 相同" /></a>
            </div>
            <h3 id="使用方式">使用方式</h3>
            <div class="block">
                <a href="./img/usage.png"><img src="./img/usage.png" alt="" /></a>
            </div>
        </div>

        <h2 id="實驗">實驗</h2>
        <ul class="directory">
            <li><a href="#基本模型">基本模型</a></li>
            <li><a href="#實驗-A">實驗 A</a></li>
            <li><a href="#實驗-B">實驗 B</a></li>
            <li><a href="#實驗-C">實驗 C</a></li>
            <li><a href="#實驗-D">實驗 D</a></li>
            <li><a href="#實驗-E">實驗 E</a></li>
        </ul>
        <div class="block">
            <h3 id="基本模型">基本模型</h3>
            <div class="block">
                <a href="./img/experiment-base.png"><img src="./img/experiment-base.png" alt="Transformer 架構圖" /></a>
                <table>
                    <tr>
                        <th>N</th>
                        <td>encoder&decoder 的數量</td>
                    </tr>
                    <tr>
                        <th>d<sub>model</sub></th>
                        <td>embedding length (前面的 d<sub>embedding</sub>)</td>
                    </tr>
                    <tr>
                        <th>d<sub>ff</sub></th>
                        <td>encoder&decoder 中，Feed Forward 隱藏層神經元數量</td>
                    </tr>
                    <tr>
                        <th>h</th>
                        <td>Multi-Head Attention 的 head 個數</td>
                    </tr>
                    <tr>
                        <th>d<sub>k</sub></th>
                        <td>Q<sub>input</sub>、K<sub>input</sub> 經過線性轉換得到之 Q、K 的深度(維度)</td>
                    </tr>
                    <tr>
                        <th>d<sub>v</sub></th>
                        <td>V<sub>input</sub> 經過線性轉換得到之 V 的深度(維度)</td>
                    </tr>
                    <tr>
                        <th>P<sub>drop</sub></th>
                        <td>Dropout 設定的機率</td>
                    </tr>
                    <tr>
                        <th><span class=text-lg>ϵ</span><sub>ls</sub></th>
                        <td>label smoothing 中的機率雜訊</td>
                    </tr>
                </table>
            </div>
            <h3 id="實驗-A">實驗 A</h3>
            <div class="block">
                <p>
                    在固定的 d<sub>model</sub> 下，<strong>增加</strong> Head 的數量對<strong>提升正確率</strong>是有幫助的，<br />
                    但是，<br />
                    當<strong>過量增加</strong>時反而會抑制單一 Head 的表達能力，<strong>使正確率開始下降</strong>。
                </p>
                <a href="./img/experiment-a.png"><img src="./img/experiment-a.png" alt="Transformer 架構圖" /></a>
            </div>
            <h3 id="實驗-B">實驗 B</h3>
            <div class="block">
                <a href="./img/experiment-b.png"><img src="./img/experiment-b.png" alt="Transformer 架構圖" /></a>
            </div>
            <h3 id="實驗-C">實驗 C</h3>
            <div class="block">
                <a href="./img/experiment-c.png"><img src="./img/experiment-c.png" alt="Transformer 架構圖" /></a>
            </div>
            <h3 id="實驗-D">實驗 D</h3>
            <div class="block">
                <a href="./img/experiment-d.png"><img src="./img/experiment-d.png" alt="Transformer 架構圖" /></a>
            </div>
            <h3 id="實驗-E">實驗 E</h3>
            <div class="block">
                <a href="./img/experiment-e.png"><img src="./img/experiment-e.png" alt="Transformer 架構圖" /></a>
            </div>
        </div>
    </div>
    <!-- </div> -->
</body>

</html>